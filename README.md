# LangChain RAG

A Retrieval-Augmented Generation (RAG) question-answering system built with LangChain, supporting PDF file upload, document vector-based retrieval, and integration with large language models to generate context-aware answers.

## ğŸŒŸ Highlights

- âœ… Built on LangChain v0.3 architecture
- ğŸ“„ Supports PDF document upload and content embedding
- ğŸ” Integrated with FAISS for efficient document retrieval
- ğŸ¤– Compatible with OpenAI / DeepSeek and other LLM providers
- ğŸŒ FastAPI backend for easy deployment and integration

---

## ğŸ“ Project Structure

```
langchain_rag/
â”‚
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ main.py # FastAPI entry point
â”‚ â”œâ”€â”€ loader.py # read and process pdf file
â”‚ â”œâ”€â”€ embedder.py # Embedding model and vector store integration
â”‚ â”œâ”€â”€ config.py # Load project environment variables
â”‚
â”œâ”€â”€ .env # Project environment variables
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Fast start

### 1. install dependencies

```bash
pip install -r requirements.txt
mv .env.template .env
# edit .env and fill in your LLM's api key and base url
```

### 2. Start service

```bash
uvicorn main:app --reload
```

### 3. upload

Upload Documents and Ask Questions
Use the /upload-pdf/ endpoint to upload a PDF file with field name 'file', then use the /query/ endpoint to ask questions. The system will generate answers based on the document content. For example:
http://localhost:8000/query?question=hi

### ğŸ§© Tech Stack

LangChain v0.3+

FAISS vector store

Sentence-Transformers embedding models

FastAPI + Uvicorn

Pydantic + Python 3.10+
